{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:00:48.871101Z",
     "iopub.status.busy": "2025-04-06T16:00:48.870773Z",
     "iopub.status.idle": "2025-04-06T16:01:47.451732Z",
     "shell.execute_reply": "2025-04-06T16:01:47.450144Z",
     "shell.execute_reply.started": "2025-04-06T16:00:48.871071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ QA:\n",
      " Q: What is the capital of France?\n",
      "A: Paris.\n",
      "\n",
      "Q. What does the word “French” mean?A. It means the people of the country. And the French people are very good at what they do. They are polite, and they are nice to each other. But they have a bad habit of speaking in French. In fact, they speak French to one another. The French are a very polite people. I would like to be French someday. (Laughter.)\n",
      "(Applause.) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def generate_text(model_name, prompt, generation_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        **generation_config\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Task-specific generation settings\n",
    "qa_config = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 40,\n",
    "    \"do_sample\": True,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"no_repeat_ngram_size\": 2\n",
    "}\n",
    "\n",
    "prompt = \"Q: What is the capital of France?\\nA:\"\n",
    "\n",
    "output = generate_text(\"EleutherAI/gpt-neo-2.7B\", prompt, qa_config)\n",
    "print(\"❓ QA:\\n\", output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
